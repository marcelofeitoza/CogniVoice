# -*- coding: utf-8 -*-
"""pln_CogniVoice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10otP1DMmnQc-5po5uEDWaNRorV-WqO40

# Processamento de linguagem natural para modelo de busca por voz.

# 1.0 Instalação

"""## 1.1 Importação de bibliotecas"""

import pandas as pd
import numpy as np

from unidecode import unidecode

import spacy

#nltk para pre proscesamento e tokenização
import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
import re

from nltk.stem import WordNetLemmatizer

nltk.download('rslp')

stemmer = nltk.stem.RSLPStemmer()

# Pacote nlp para portugues
nlp = spacy.cli.download('pt_core_news_sm')
nlp = spacy.load('pt_core_news_sm')

#Biblioteca usada para consultar uma URL
import requests
import urllib.request
from urllib.request import urlopen
from urllib.error import HTTPError

#Biblioteca usada analisar os dados retornados da url
from bs4 import BeautifulSoup


#importação de keras, para execução do modelo e criação do dicionario
from keras.preprocessing.text import Tokenizer

#setup w2vec
import gensim
from scipy.spatial.distance import cosine
from gensim.models import KeyedVectors

# Biblioteca para funções matemáticas
import math
import os

"""# 2.0 Importação e compreensão dos dados

##2.1 DataFrame criado manualmente que se assemelha ao do banco de dados para realização de testes
"""
diretorio_atual = os.path.dirname(os.path.abspath(__file__))

# Nome do arquivo CSV que você deseja carregar
nome_arquivo = "ibm_db.csv"

# Crie o caminho completo para o arquivo CSV
caminho_arquivo = os.path.join(diretorio_atual, nome_arquivo)

dados_csv = []

# Abra o arquivo CSV e leia seus conteúdos
with open(caminho_arquivo, mode='r', newline='') as arquivo_csv:
    leitor_csv = csv.reader(arquivo_csv)
    for linha in leitor_csv:
        # Adicione cada linha à lista de dados_csv
        dados_csv.append(linha)

df_url = pd.read_csv(dados_csv)

df_url

df_url.info()

"""# 3.0 Tratamento dos dados
"""

"""## 3.1 Remoção de acentos

### 3.1.1 Definição da função
"""

def remocaoAcentos(text):
    return unidecode(text)

"""## 3.2 Tratamento de letras maiusculas

### 3.2.1 Definição da função
"""

def maiusculas(dados):
    if isinstance(dados, pd.DataFrame):
        return dados.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    elif isinstance(dados, str):
        return dados.lower()
    else:
        return dados

def lowercase_text(text):
  return text.lower()

"""## 3.3 Tratamento do veiculo da noticia

### 3.3.1 Definição da função
"""

def clean_title(match_comp):
    pattern = re.compile(r'^(.*?)\s*(-|\||por)\s*.*$')
    match = pattern.match(match_comp)
    return match.group(1) if match else match_comp

"""## 3.4 Limpeza pontuação

### 3.4.1 Definição da função
"""

def remove_marks(text):
    if isinstance(text, str): #verifica se o que está sendo processado é uma string
        return re.sub(r'[^\w\s]|_', '', text) #substituir todos os caracteres especiais por uma string vazia
    else:
        return text

"""## 3.5 Remoção de aspas

### 3.5.1 Definição da função
"""

def remove_aspas(column_or_text):
    if isinstance(column_or_text, pd.Series):  # Verifica se é uma coluna de DataFrame
        return column_or_text.apply(remove_aspas)
    elif isinstance(column_or_text, str):  # Verifica se é uma string
        return column_or_text.replace('"', '')
    else:
        return column_or_text  # Retorna outros tipos de dados sem modificação


"""## 3.6 Remoção de espaços duplicados

### 3.6.1 Definição da função
"""

def remove_duplicate_spaces(text):
    # Substitui sequências de espaços em branco por um único espaço
    cleaned_text = re.sub(r'\s+', ' ', text)
    return cleaned_text.strip()

"""## 3.7 Remoção de números isolados

### 3.7.1 Definição da função
"""

def remove_isolated_numbers(text):
    # Substitui números isolados por um espaço em branco
    cleaned_text = re.sub(r'\b\d+\b', '', text)
    return cleaned_text.strip()

"""# 4.0 Web Scraping

## 4.1 Extração do titulo

### 4.1.1 Definição da função
"""

def extrair_titulo(url):
    print(url)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'
    }

    try:
        req = urllib.request.Request(url, headers=headers)
        page = urllib.request.urlopen(req)

        # response = requests.get(url, headers=headers)
        # response.raise_for_status()

        soup = BeautifulSoup(page, 'html.parser')
        title_new = soup.title.string
        title_new = clean_title(title_new)
        return title_new
    except HTTPError as e:
        print(f"Erro ao acessar a URL {url}: {e}")
        return ""  # Retorna um título vazio em caso de erro

"""## 4.2 Extração da noticia na integra e tratamento do dimensionamento

### 4.2.1 Definição de função
"""

def remove_tags(url):
    try:
        useless_tittle = extrair_titulo(url)
        useless_tittle = remocaoAcentos(useless_tittle)
        useless_tittle = maiusculas(useless_tittle)
        tittle = clean_title(useless_tittle)

        # parse html content
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'
        }

        # req = urllib.request.Request(url, headers=headers)
        # page = urllib.request.urlopen(req)

        page = requests.get(url, headers=headers)
        page.raise_for_status()

        soup = BeautifulSoup(page.text, "html.parser")

        for data in soup(['style', 'script', 'title']):
            # Remove as tags
            data.decompose()

        # return data by retrieving the tag content
        all_text = ' '.join(soup.stripped_strings)

        all_text = maiusculas(all_text)
        all_text = remocaoAcentos(all_text)

        quebra = all_text.find(tittle)

        if quebra != -1:  # verifica encontrou a quebra
            all_text = all_text[quebra:]
            all_text = remove_marks(all_text)
            all_text = remove_aspas(all_text)
            all_text = remove_duplicate_spaces(all_text)
            all_text = remove_isolated_numbers(all_text)
            return all_text
        else:
            all_text = remove_marks(all_text)
            all_text = remove_aspas(all_text)
            all_text = remove_duplicate_spaces(all_text)
            all_text = remove_isolated_numbers(all_text)
            return all_text
    except Exception as e:
        print(f"Erro ao processar a URL {url}: {e}")
        return ""  # Retorna uma string vazia em caso de erro

"""# 5.0 Extração dos dados

## 5.1 Extração da data

### 5.1.1 Definição da função
"""

date_regex = [
  r'(\d{1,2} de [a-zA-Z]+ de \d{4})',
  r'(\d{1,2}/\d{1,2}/\d{2,4})',
  r'(\d{1,2} [a-zA-Z]+ \d{2,4})'
]

def extract_dates(row):
    quebra = row['tag_removed'].find(row['titulo'])
    for regex in date_regex:
        match = re.search(regex, row['tag_removed'][quebra:])
        if match:
            return match.group(1)
    return None

"""## 5.2 Extração da fonte

### 5.2.1 Definição da função
"""

def extrair_dominio(url):
    regex_url = r'https://(.*?)/'
    fonte = re.findall(regex_url, url)
    if fonte:
        return fonte[0]
    else:
        return None

"""## 5.3 Extração das entidades nomeadas

### 5.3.1 Definição da função
"""

def named_entities_filtred(coluna, filtro):
  results = []
  for linha in coluna:
    # print(linha)
    doc = nlp(linha)
    org_entities = set(ent.text for ent in doc.ents if ent.label_ == filtro)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    results.append(org_entities)
    # print("teste",results)
  return results

def named_entities(coluna):
  results = []
  for linha in coluna:
    # print(linha)
    doc = nlp(linha)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    results.append(entities)
    # print("teste",results)
  return results

"""# 6.0 Aplicação das tecnicas de PLN

## 6.1 Stop Words

### 6.1.1 Definição da função
"""

def stopWords(dados):
    if isinstance(dados, str):
        dados = [dados]

    resultado = [' '.join([token.text for token in nlp(texto) if not token.is_stop]) for texto in dados]

    return resultado

"""## 6.2 Lematização

### 6.2.1 Definição da função
"""

def lematizar(dados):
    resultado = []
    for texto in dados:
        # Processa o texto com o modelo do Spacy
        doc = nlp(texto)
        # Lematiza o documento
        lemmas = [token.lemma_ for token in doc]
        # Junta tudo de volta a um texto
        textoLemmatizado = ' '.join(lemmas)
        resultado.append(textoLemmatizado)
    return resultado

"""## 6.3 Tokenização

### 6.3.1 Definição da função
"""

def tokenizar_texto(texto):
    doc = nlp(texto)
    tokens = [token.text for token in doc]
    return tokens

"""# 7.0 Pipeline de pré processamento

## 7.1 Importação de um novo dataframe
"""

df_pipeline = pd.read_csv('/content/drive/Shareddrives/CogniVoice/ibm_db.csv')

df_pipeline

"""## 7.2 Definição da função pipeline para dataframe
"""

def pipeline_dataframe(dados, colum_name):
  #Cria um dataframe vazio para armazenar os dados tratados
  treated_data =  pd.DataFrame()

  #Busca pela url o site, faz o tratamento do html e retorna o texto da noticia
  treated_data["data"] = dados[colum_name].apply(remove_tags)

  treated_data["data"] = treated_data["data"].apply(remove_duplicate_spaces)

  #Remove possiveis acentos da tabela
  treated_data = treated_data.applymap(remocaoAcentos)

  treated_data = treated_data.applymap(lowercase_text)

  treated_data["data"] = stopWords(treated_data["data"])

  treated_data["data"] = lematizar(treated_data["data"])

  treated_data["data"] = treated_data["data"].apply(tokenizar_texto)

  return treated_data

"""## 7.3 Definição da função pipeline entrada do usuario
"""

def pipeline_speech_to_text(text):
  array_phrase = []

  text = remocaoAcentos(text)

  text = lowercase_text(text)

  text = remove_marks(text)

  text = stopWords(text)

  text = lematizar(text)

  text = tokenizar_texto(str(text)[1:-1].strip("'"))

  array_phrase.append(text)

  return array_phrase

"""# 12.0 Definição da função de embedding
"""

from sentence_transformers import SentenceTransformer

#função para vetorização
def Vetorizacao(dataframe,modelo):
  # Calcula os embeddings para a coluna de texto
  embeddings_liz = modelo.encode(dataframe.tolist())

  # Cria um novo DataFrame com os embeddings
  embeddings_df = pd.DataFrame(embeddings_liz)

  return embeddings_df

"""# 12.0 Transformers vetorização
"""

vetorizadorTransformers = SentenceTransformer('distiluse-base-multilingual-cased')

"""# 14.0 Similaridade do coseno

A função de similaridade do cosseno é uma métrica usada para medir a semelhança entre dois vetores em um espaço multidimensional. Ela calcula o coseno do ângulo entre esses vetores. Quanto mais próximos os vetores estiverem um do outro, menor será o ângulo e, portanto, maior será o valor do cosseno, indicando maior similaridade. Por outro lado, se os vetores estiverem em direções opostas, o cosseno será -1, indicando dissimilaridade completa. O valor da similaridade do cosseno varia de -1 (completamente dissimilar) a 1 (completamente similar), com 0 indicando independência.

Vamos empregar a função de similaridade do cosseno para comparar o input com todas as notícias armazenadas no banco de dados, que também foram vetorizadas. Com os resultados obtidos através dessa função, identificaremos quais notícias são mais semelhantes ao input, permitindo-nos apresentar as notícias relevantes ao usuário.
"""

def calculate_similiarity(vectors: list, phrase_vector: list) -> float:
    text_similarity_array = []

    if str(type(phrase_vector)) != "<class 'numpy.ndarray'>":
      phrase_vector = phrase_vector.values
    else:
      pass

    for index, row in vectors.iterrows():
        np_vector_row = row.values
        np_vector_text = phrase_vector

        product_sum = np.sum(np_vector_row * np_vector_text)

        squared_frase_um = np.square(np_vector_row)
        squared_frase_dois = np.square(np_vector_text)

        sum_of_squares_array1 = np.sum(squared_frase_um)
        sum_of_squares_array2 = np.sum(squared_frase_dois)

        modulo = math.sqrt(sum_of_squares_array1) * math.sqrt(sum_of_squares_array2)

        text_similarity = product_sum / modulo

        text_similarity_array.append(text_similarity)


    return text_similarity_array

"""#17.0 Funções auxiliares para a API

##17.1 Retorna os indexs das noticias com maiores similaridades identificadas
"""

def encontrar_maiores(array):
    maiores = []
    for i, numero in enumerate(array):
        if numero > 0.85:
            maiores.append([numero, i])

    maiores.sort(reverse=True)

    if len(maiores) >= 3:
        return maiores[:3]
    elif len(maiores) > 0:
        return maiores
    else:
        return 'Nao ha nenhuma noticia correspondente em nosso banco de dados.'

"""##17.2 Retorna o conteúdo das noticias, buscando em nosso dataframe
"""

import json

def encontrar_noticias(indices, dataframe, coluna):
    noticias = []

    if str(type(indices)) == "<class 'str'>":
      return 'Nao ha nenhuma noticia correspondente em nosso banco de dados.'

    for i in indices:
        noticias.append(dataframe[coluna][i[1]])

    noticias_dict = {
        "noticia1": noticias[0] if len(noticias) > 0 else None,
        "noticia2": noticias[1] if len(noticias) > 1 else None,
        "noticia3": noticias[2] if len(noticias) > 2 else None
    }

    json_data = json.dumps(noticias_dict)

    return json_data

"""# 18.0 Processamento do database oficial em nossas pipelines e funções
"""

url = 'http://10.254.18.13:3001/v1/url/getall'
response = requests.get(url)

if response.status_code == 200:
    data = response.json()
else:
    print('Erro ao obter os dados da API')

df_news_database = pd.DataFrame(data)

df_news_database

df_news_database_vectors = pipeline_dataframe(df_news_database, "kkkk")
df_news_database_vectors

df_full_news_transformers = Vetorizacao(df_news_database_vectors['kkkk'], vetorizadorTransformers)

df_news = pd.DataFrame()
df_news['all_news'] = df_url['url_noticia'].apply(remove_tags)

"""# 19.0 Flask API"""

from flask import Flask, request, jsonify
import requests
import pandas as pd

app = Flask(__name__)

# Rota de saúde
@app.route("/health", methods=['GET'])
def health_check():
    return "API está rodando sem erros", 200

# Rota para processar a entrada de voz
@app.route("/process_audio", methods=['POST'])
def process_audio():
    try:
        data = request.get_json()
        if data is not None and 'text' in data:
            text_value = data['text']

            text_value = pipeline_speech_to_text(text_value)

            vector_text = vetorizadorTransformers.encode(text_value)

            news_response = calculate_similiarity(df_full_news_transformers,vector_text)

            news_index = encontrar_maiores(news_response)

            news_response = encontrar_noticias(news_index, df_news, 'all_news')

            return "Sucesso", 200
        else:
            return jsonify({"erro": "Dados JSON inválidos"}), 400
    except Exception as e:
        return jsonify({"erro": str(e)}), 500

@app.route("/call_external_api", methods=['GET'])
def update_news():
    url = 'http://10.254.18.13:3001/v1/url/getall'
    response = requests.get(url)

    if response.status_code == 200:
        data = response.json()
    else:
        print('Erro ao obter os dados da API')

    df_news_database = pd.DataFrame(data)

    return "acionada em", 200

if __name__ == '__main__':
    app.run(host='44.214.133.236', port=3000)
