# -*- coding: utf-8 -*-
"""pln_CogniVoice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10otP1DMmnQc-5po5uEDWaNRorV-WqO40

# Processamento de linguagem natural para modelo de busca por voz.

# 1.0 Instalação

Para garantir o bom funcionamento do código de processamento de texto, análise de linguagem natural, vetorização, cálculo de similaridade e aprendizado de máquina para classificação, é imprescindível realizar as seguintes instalações e importações de bibliotecas. É de extrema importância que essas instalações estejam corretamente configuradas e que a máquina na qual o código será executado atenda aos requisitos necessários para seu funcionamento adequado.

## 1.1 Instalação da bibliotecas
"""

# !pip install unidecode
# !pip install spacy
# !pip install requests

"""## 1.2 Importação de bibliotecas"""

import pandas as pd
import numpy as np

from unidecode import unidecode

import spacy

#nltk para pre proscesamento e tokenização
import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
import re

from nltk.stem import WordNetLemmatizer

nltk.download('rslp')

stemmer = nltk.stem.RSLPStemmer()

# Pacote nlp para portugues
nlp = spacy.cli.download('pt_core_news_sm')
nlp = spacy.load('pt_core_news_sm')

#Biblioteca usada para consultar uma URL
import requests
import urllib.request
from urllib.request import urlopen
from urllib.error import HTTPError

#Biblioteca usada analisar os dados retornados da url
from bs4 import BeautifulSoup


#importação de keras, para execução do modelo e criação do dicionario
from keras.preprocessing.text import Tokenizer

#setup w2vec
import gensim
from scipy.spatial.distance import cosine
from gensim.models import KeyedVectors

# Biblioteca para funções matemáticas
import math


# Google Drive
from google.colab import drive
drive.mount('/content/drive')

"""# 2.0 Importação e compreensão dos dados

Nesta seção, realizamos a importação de dois dataframes. O dataframe é criado manualmente e possui informações semelhantes às que estão contidas no primeiro. Este dataframe é exclusivamente destinado aos testes de todas as funções que desenvolvemos ao longo deste notebook.

##2.1 DataFrame criado manualmente que se assemelha ao do banco de dados para realização de testes
"""

df_url = pd.read_csv('./ibm_db.csv')

df_url

df_url.info()

"""# 3.0 Tratamento dos dados

Nesta etapa, inicialmente criamos um dataframe manual contendo 12 frases aleatórias que servirão como entradas e saídas de teste para nossas funções. As funções que definimos nesta seção estão todas relacionadas ao processamento de texto, visando melhorar a compreensão por parte da máquina. É relevante destacar que, ao final desta seção, apresentamos como resposta os resultados de todas as funções aplicadas ao dataframe criado no início.

A seguir, listamos todas as funções:

*   Remoção de acentos
*   Tratamento de letras maiusculas
*   Tratamento do veiculo da noticia
*   Limpeza pontuação
*   Remoção de aspas
*   Remoção de espaços duplicados
*   Remoção de números isolados

Essas funções desempenham um papel fundamental na preparação e limpeza do texto, tornando-o mais adequado para análise posterior e garantindo um melhor desempenho dos modelos de aprendizado de máquina que serão aplicados.
"""

dados_dataframe = [
    "Esta é a primeira frase.",
    "Aqui vem a segunda frase com palavras de parada.",
    "E a terceira frase.",
    "Quarta frase é necessário o uso de acentos e pontuações",
    "Teste para pontuação (virgulas, `""´ e dois pontos:)",
    "Exemplo de titulo 1  - Inforchannel",
    "Exemplo de titulo 2 | Voxel",
    "Exemplo de titulo 3 Por Investing.com",
    "  Isso  é um   exemplo   ",
    "  Espaços   múltiplos     ",
    "oi, tudo 3 bem 4 com voce?",
    "isso 76 apresenta 12 número isolados"
]

test_dataframe = pd.DataFrame(dados_dataframe)

test_dataframe

"""## 3.1 Remoção de acentos

Essa função é fundamental para melhorar o entendimento do texto pela máquina. Ela transforma todas as letras acentuadas em suas formas não acentuadas. Por exemplo, converte "é" em "e" e "nós" em "nos". Isso é importante porque simplifica o texto e evita que a presença de acentos atrapalhe o processamento.

### 3.1.1 Definição da função
"""

def remocaoAcentos(text):
    return unidecode(text)

"""### 3.1.2 Teste da função"""

test_dataframe = test_dataframe.applymap(remocaoAcentos)

"""## 3.2 Tratamento de letras maiusculas

Transformar todas as letras em minúsculas é uma prática essencial para garantir a uniformidade do texto. Isso é necessário para que a máquina não diferencie letras maiúsculas de minúsculas, o que poderia prejudicar a análise do texto.

### 3.2.1 Definição da função
"""

def maiusculas(dados):
    if isinstance(dados, pd.DataFrame):
        return dados.applymap(lambda x: x.lower() if isinstance(x, str) else x)
    elif isinstance(dados, str):
        return dados.lower()
    else:
        return dados

def lowercase_text(text):
  return text.lower()

"""### 3.2.2 Teste da função"""

test_dataframe = test_dataframe.applymap(lowercase_text)

test_dataframe

"""## 3.3 Tratamento do veiculo da noticia

Usando expressões regulares (regex), esta função remove partes do título da notícia que não são relevantes. Por exemplo, em "IBM cria projeto com Inteli - Por Fulano", a parte " - Por Fulano" é excluída do título. Essa função ajuda a limpar o texto e é útil para filtrar notícias por título, se necessário.

### 3.3.1 Definição da função
"""

def clean_title(match_comp):
    pattern = re.compile(r'^(.*?)\s*(-|\||por)\s*.*$')
    match = pattern.match(match_comp)
    return match.group(1) if match else match_comp

"""### 3.3.2 Teste da função"""

test_dataframe[0] = test_dataframe[0].apply(clean_title)

"""## 3.4 Limpeza pontuação

Esta função elimina todos os caracteres não alfanuméricos de uma string, como vírgulas, pontos de interrogação e exclamação. Isso é importante para o processamento de texto, pois tais caracteres podem aumentar o tempo de processamento e complicar a análise.

### 3.4.1 Definição da função
"""

def remove_marks(text):
    if isinstance(text, str): #verifica se o que está sendo processado é uma string
        return re.sub(r'[^\w\s]|_', '', text) #substituir todos os caracteres especiais por uma string vazia
    else:
        return text

"""### 3.4.2 Teste da função"""

test_dataframe[0] = test_dataframe[0].apply(remove_marks)

"""## 3.5 Remoção de aspas

A remoção de aspas excessivas dentro de uma string é crucial para evitar problemas com modelos de vetorização que serão explicados posteriormente no código. Por exemplo, em "oi, tudo ''' bem?", as aspas repetidas são excluídas para evitar erros nos modelos.

### 3.5.1 Definição da função
"""

def remove_aspas(column_or_text):
    if isinstance(column_or_text, pd.Series):  # Verifica se é uma coluna de DataFrame
        return column_or_text.apply(remove_aspas)
    elif isinstance(column_or_text, str):  # Verifica se é uma string
        return column_or_text.replace('"', '')
    else:
        return column_or_text  # Retorna outros tipos de dados sem modificação

"""### 3.5.2 Teste da função"""

test_dataframe = test_dataframe.apply(remove_aspas)

"""## 3.6 Remoção de espaços duplicados

Esta função elimina espaços em excesso dentro de uma string. Por exemplo, em "oi,      tudo bem?", os espaços duplicados são removidos. Isso é importante para evitar problemas com modelos de vetorização que serão explicados posteriormente no código.

### 3.6.1 Definição da função
"""

def remove_duplicate_spaces(text):
    # Substitui sequências de espaços em branco por um único espaço
    cleaned_text = re.sub(r'\s+', ' ', text)
    return cleaned_text.strip()

"""### 3.6.2 Teste da função"""

test_dataframe[0] = test_dataframe[0].apply(remove_duplicate_spaces)

"""## 3.7 Remoção de números isolados

Excluir números isolados de uma string, como em "oi, tudo 23 bem?", é importante para evitar erros nos modelos de vetorização que serão explicados mais adiante no código. A presença de números soltos pode complicar o processamento do texto.

### 3.7.1 Definição da função
"""

def remove_isolated_numbers(text):
    # Substitui números isolados por um espaço em branco
    cleaned_text = re.sub(r'\b\d+\b', '', text)
    return cleaned_text.strip()

"""### 3.7.2 Definição da função"""

test_dataframe[0] = test_dataframe[0].apply(remove_isolated_numbers)

"""## 3.8 Dataframe resultante da seção"""

test_dataframe

"""# 4.0 Web Scraping

Nesta seção, utilizaremos técnicas de Web Scraping para extrair e processar dados a partir da URL do nosso banco de dados. Como temos várias URLs, o processo consiste em fazer uma requisição ao site, obter o seu HTML e, em seguida, remover todas as tags que não serão utilizadas, como style e script, entre outras.

Além disso, nesta seção, realizaremos o tratamento para eliminar conteúdo não relevante na notícia, incluindo o conteúdo que aparece antes e depois da notícia, uma vez que coletamos todo o conteúdo da página HTML. É importante ressaltar que, ao final desta seção, apresentaremos o resultado em forma de um DataFrame.

## 4.1 Extração do titulo

Este código define uma função chamada "extrair_titulo" que recebe uma URL como entrada. Ela faz o seguinte:



1.   Define informações do navegador.
2.   Tenta acessar a URL da web usando essas informações.
3.   Extrai o título da página web.
4.   Retorna o título extraído ou uma string vazia em caso de erro.

Basicamente, a função recupera o título de uma página da web quando fornecida uma URL.

### 4.1.1 Definição da função
"""

def extrair_titulo(url):
    print(url)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'
    }

    try:
        req = urllib.request.Request(url, headers=headers)
        page = urllib.request.urlopen(req)

        # response = requests.get(url, headers=headers)
        # response.raise_for_status()

        soup = BeautifulSoup(page, 'html.parser')
        title_new = soup.title.string
        title_new = clean_title(title_new)
        return title_new
    except HTTPError as e:
        print(f"Erro ao acessar a URL {url}: {e}")
        return ""  # Retorna um título vazio em caso de erro

"""### 4.1.2 Teste da função"""

df_url['titulo'] = df_url['url_noticia'].apply(extrair_titulo)

df_url['titulo']

"""## 4.2 Extração da noticia na integra e tratamento do dimensionamento

Este código define uma função chamada "remove_tags" que recebe uma URL como entrada. A função faz o seguinte:


1.   Chama a função "extrair_titulo" para obter o título da página da web a partir da URL.
2.   Realiza uma série de operações de processamento de texto no título obtido, incluindo remoção de acentos, conversão para letras maiúsculas e possivelmente uma função "clean_title".
3.   A função emula uma solicitação HTTP à URL usando informações de cabeçalho do navegador.
4.   Usa a biblioteca BeautifulSoup para analisar o conteúdo HTML da página web.
5.   Remove as tags HTML do conteúdo, incluindo tags de estilo, script e título.
6.   Extrai o texto remanescente da página, removendo espaços extras.
7.   Procura pelo título previamente processado dentro do texto e o recorta a partir desse ponto, se encontrado.
8.   Realiza mais operações de limpeza no texto, como remoção de pontuações, aspas, espaços duplicados e números isolados.
9.   Retorna o texto processado resultante.


Se ocorrer algum erro durante esse processo, a função imprime uma mensagem de erro e retorna uma string vazia. Em resumo, a função "remove_tags" extrai o texto de uma página da web, limpa-o e o retorna, com base no título da página.

### 4.2.1 Definição de função
"""

def remove_tags(url):
    try:
        useless_tittle = extrair_titulo(url)
        useless_tittle = remocaoAcentos(useless_tittle)
        useless_tittle = maiusculas(useless_tittle)
        tittle = clean_title(useless_tittle)

        # parse html content
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'
        }

        # req = urllib.request.Request(url, headers=headers)
        # page = urllib.request.urlopen(req)

        page = requests.get(url, headers=headers)
        page.raise_for_status()

        soup = BeautifulSoup(page.text, "html.parser")

        for data in soup(['style', 'script', 'title']):
            # Remove as tags
            data.decompose()

        # return data by retrieving the tag content
        all_text = ' '.join(soup.stripped_strings)

        all_text = maiusculas(all_text)
        all_text = remocaoAcentos(all_text)

        quebra = all_text.find(tittle)

        if quebra != -1:  # verifica encontrou a quebra
            all_text = all_text[quebra:]
            all_text = remove_marks(all_text)
            all_text = remove_aspas(all_text)
            all_text = remove_duplicate_spaces(all_text)
            all_text = remove_isolated_numbers(all_text)
            return all_text
        else:
            all_text = remove_marks(all_text)
            all_text = remove_aspas(all_text)
            all_text = remove_duplicate_spaces(all_text)
            all_text = remove_isolated_numbers(all_text)
            return all_text
    except Exception as e:
        print(f"Erro ao processar a URL {url}: {e}")
        return ""  # Retorna uma string vazia em caso de erro

"""### 4.2.2 Teste da função"""

df_url['tag_removed'] = df_url['url_noticia'].apply(remove_tags)

"""## 4.3 Dataframe resultante da seção"""

df_url

"""# 5.0 Extração dos dados

No processo de extração de dados, nosso principal objetivo é coletar informações adicionais que tornem mais simples para o usuário o filtro de notícias quando duas ou mais notícias correspondem no banco de dados.

A seguir, detalharemos quais informações estamos obtendo da notícia completa, conforme discutido na última seção sobre Web Scraping.



1.   Extração da Data: Este processo envolve a extração da data em que a notícia foi publicada.
2.   Extração da Fonte: Aqui, estamos coletando a fonte na qual a notícia foi originalmente veiculada.
3.   Extração de Entidades Nomeadas: Realizamos a extração de todas as organizações e pessoas mencionadas na notícia, a fim de utilizá-las como palavras-chave para facilitar as buscas dos usuários.

É relevante destacar que as duas primeiras funções utilizam expressões regulares (regex) para realizar a extração, enquanto a última emprega uma técnica de Processamento de Linguagem Natural (PLN) para realizar o mesmo procedimento. Os resultados dessas extrações podem ser encontrados ao final desta seção.

## 5.1 Extração da data

### 5.1.1 Definição da função
"""

date_regex = [
  r'(\d{1,2} de [a-zA-Z]+ de \d{4})',
  r'(\d{1,2}/\d{1,2}/\d{2,4})',
  r'(\d{1,2} [a-zA-Z]+ \d{2,4})'
]

def extract_dates(row):
    quebra = row['tag_removed'].find(row['titulo'])
    for regex in date_regex:
        match = re.search(regex, row['tag_removed'][quebra:])
        if match:
            return match.group(1)
    return None

"""### 5.1.2 Teste da função"""

df_url['data'] = df_url.apply(extract_dates, axis=1)

"""## 5.2 Extração da fonte

### 5.2.1 Definição da função
"""

def extrair_dominio(url):
    regex_url = r'https://(.*?)/'
    fonte = re.findall(regex_url, url)
    if fonte:
        return fonte[0]
    else:
        return None

"""### 5.2.2 Teste da função"""

df_url['fonte'] = df_url['url_noticia'].apply(extrair_dominio)

"""## 5.3 Extração das entidades nomeadas

### 5.3.1 Definição da função
"""

def named_entities_filtred(coluna, filtro):
  results = []
  for linha in coluna:
    # print(linha)
    doc = nlp(linha)
    org_entities = set(ent.text for ent in doc.ents if ent.label_ == filtro)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    results.append(org_entities)
    # print("teste",results)
  return results

def named_entities(coluna):
  results = []
  for linha in coluna:
    # print(linha)
    doc = nlp(linha)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    results.append(entities)
    # print("teste",results)
  return results

"""### 5.3.2 Teste da função"""

teste_entities_df = pd.DataFrame()

df_url["empresas"] = named_entities_filtred(df_url['tag_removed'], "ORG")
df_url["pessoas"] = named_entities_filtred(df_url['tag_removed'], "PER")
teste_entities_df["Entidades"] = named_entities(df_url['tag_removed'])

"""## 5.4 Dataframe resultante da seção"""

df_url

"""# 6.0 Aplicação das tecnicas de PLN

Agora que concluímos o tratamento completo do texto, é hora de aplicar algumas técnicas de Processamento de Linguagem Natural (PLN) para facilitar a vetorização do texto e melhorar a compreensão por parte da máquina.

As técnicas de PLN que foram cuidadosamente escolhidas e aplicadas incluem:

1.   Stop Words
2.   Lematização
3.   Tokenização

É importante destacar que, durante nossa pesquisa e experimentação, consideramos a possibilidade de utilizar outras técnicas de PLN. No entanto, concluímos que muitas delas não proporcionavam melhorias substanciais no resultado final. Portanto, optamos por manter um conjunto mais simples e eficaz de técnicas de PLN para esta aplicação específica.

Ao final desta seção, você encontrará os resultados finais das aplicações dessas técnicas de PLN. Esses resultados demonstrarão como as etapas de pré-processamento melhoraram a qualidade dos dados e tornaram o texto mais adequado para análise por máquinas.

## 6.1 Stop Words

Eliminamos as palavras de parada, que são palavras muito comuns na língua e geralmente não contribuem significativamente para o entendimento do texto. Essa etapa ajuda a reduzir o ruído nos dados e a melhorar o desempenho dos modelos de PLN.

### 6.1.1 Definição da função
"""

def stopWords(dados):
    if isinstance(dados, str):
        dados = [dados]

    resultado = [' '.join([token.text for token in nlp(texto) if not token.is_stop]) for texto in dados]

    return resultado

"""### 6.1.2 Teste da função"""

stopWords("um projeto realizado em parceria entre nasa e ibm pretende reduzir [...] em tempo quase que real")

df_url['tag_removed'] = stopWords(df_url['tag_removed'])

"""## 6.2 Lematização

Aplicamos a lematização para reduzir as palavras às suas formas básicas ou lemas. Isso ajuda a consolidar palavras relacionadas e simplificar a análise, já que diferentes formas de uma palavra são tratadas como uma única unidade.

### 6.2.1 Definição da função
"""

def lematizar(dados):
    resultado = []
    for texto in dados:
        # Processa o texto com o modelo do Spacy
        doc = nlp(texto)
        # Lematiza o documento
        lemmas = [token.lemma_ for token in doc]
        # Junta tudo de volta a um texto
        textoLemmatizado = ' '.join(lemmas)
        resultado.append(textoLemmatizado)
    return resultado

"""### 6.2.2 Teste da função"""

teste_string = [
    "projeto realizado parceria nasa ibm pretende reduzir tempo quase real"
]

lema_dataframe = pd.DataFrame(teste_string)

lema_dataframe[0] = lematizar(lema_dataframe[0])

lema_dataframe

df_url['tag_removed'] = lematizar(df_url['tag_removed'])

"""## 6.3 Tokenização

Dividimos o texto em tokens, que são unidades individuais, como palavras ou partes de palavras. Isso é fundamental para que a máquina compreenda a estrutura do texto e possa processá-lo de maneira eficaz.

### 6.3.1 Definição da função
"""

def tokenizar_texto(texto):
    doc = nlp(texto)
    tokens = [token.text for token in doc]
    return tokens

"""### 6.3.2 Teste da função"""

token_string = tokenizar_texto("projeto realizar parceria nasa ibm pretender reduzir tempo quase real")
token_string

df_url['tag_removed'] = df_url['tag_removed'].apply(tokenizar_texto)

"""## 6.4 Dataframe resultante da seção"""

df_url

print(df_url["titulo"][0])
print(df_url["titulo"][0])

"""# 7.0 Pipeline de pré processamento

Neste estágio, nosso objetivo é consolidar todas as funções anteriormente desenvolvidas em uma única e acessível pipeline. Com esse propósito, criamos duas pipelines de dados distintas. A primeira é responsável por processar o DataFrame contendo as URLs, enquanto a segunda lida com o tratamento de uma entrada de texto em formato de string, que representa os dados fornecidos pelo usuário por meio do backend.

1.   Pipeline de Tratamento de DataFrame:
2.   Pipeline de Tratamento de Entrada do Usuário:

Ambas as pipelines foram criadas para simplificar e otimizar a maneira como tratamos dados brutos e entradas de texto do usuário, garantindo que nossas análises e pesquisas sejam eficazes e precisas. Elas representam uma abordagem eficiente para o processamento de informações em diferentes contextos e constituem uma parte fundamental de nossa estrutura de processamento de dados.

## 7.1 Importação de um novo dataframe
"""

df_pipeline = pd.read_csv('/content/drive/Shareddrives/CogniVoice/ibm_db.csv')

df_pipeline

"""## 7.2 Definição da função pipeline para dataframe

Nossa primeira pipeline foi projetada para realizar um conjunto abrangente de etapas de processamento de dados no DataFrame que armazena as URLs. Isso inclui a extração, limpeza e transformação dos dados brutos em um formato adequado para análise e pesquisa. Essa pipeline torna a manipulação de URLs e a extração de informações relevantes um processo eficiente e organizado.
"""

def pipeline_dataframe(dados, colum_name):
  #Cria um dataframe vazio para armazenar os dados tratados
  treated_data =  pd.DataFrame()

  #Busca pela url o site, faz o tratamento do html e retorna o texto da noticia
  treated_data["data"] = dados[colum_name].apply(remove_tags)

  treated_data["data"] = treated_data["data"].apply(remove_duplicate_spaces)

  #Remove possiveis acentos da tabela
  treated_data = treated_data.applymap(remocaoAcentos)

  treated_data = treated_data.applymap(lowercase_text)

  treated_data["data"] = stopWords(treated_data["data"])

  treated_data["data"] = lematizar(treated_data["data"])

  treated_data["data"] = treated_data["data"].apply(tokenizar_texto)

  return treated_data

"""## 7.3 Definição da função pipeline entrada do usuario

A segunda pipeline foi desenvolvida para lidar com as informações fornecidas pelo usuário por meio do backend na forma de uma string. Essa entrada do usuário passa por um conjunto de processos que a preparam para análise, incluindo a aplicação das técnicas de Processamento de Linguagem Natural (PLN) mencionadas anteriormente, como a remoção de palavras de parada, lematização e tokenização. Dessa forma, garantimos que o texto do usuário esteja em uma forma adequada para compreensão e pesquisa.
"""

def pipeline_speech_to_text(text):
  array_phrase = []

  text = remocaoAcentos(text)

  text = lowercase_text(text)

  text = remove_marks(text)

  text = stopWords(text)

  text = lematizar(text)

  text = tokenizar_texto(str(text)[1:-1].strip("'"))

  array_phrase.append(text)

  return array_phrase

"""## 7.3 Teste da função"""

teste_pipeline = pipeline_dataframe(df_pipeline, "url_noticia")

phrase_teste = "Bom dia, existe algum projeto da USP em parceria com a IBM?"

phrase_teste = pipeline_speech_to_text(phrase_teste)

phrase_teste

"""## 7.4 Dataframe resultante da seção"""

teste_pipeline

"""# 8.0 Bag of Words

O BoW é uma técnica simples que converte um documento de texto em um vetor contendo a contagem de ocorrências de cada palavra no documento. Cada palavra é tratada como uma característica independente, ignorando a ordem e a estrutura das palavras no texto.

## 8.1 Definição das funções isoladas
"""

# função para vetor de recorrencia

def bagOfWords(dicionario,dados):
  salvar = dicionario.texts_to_matrix(dados, mode='count')
  return salvar

def criar_dicionario(dados):
  tokenizer = Tokenizer()
  tokenizer.fit_on_texts(dados)
  return tokenizer

"""## 8.2 Teste individual das funções"""

teste_bow = [
    "projeto,realizar,parceria,nasa,ibm,pretender,reduzir,tempo,quase,real",
    "projeto,realizar,parceria,microsoft,ibm,inteligencia,artificial,generativa,nuvem"
]

bow_dataframe = pd.DataFrame(teste_bow)

dicionarioTeste = criar_dicionario(bow_dataframe[0])
print(dicionarioTeste)
print(list(dicionarioTeste.word_index.keys()))
print(len(list(dicionarioTeste.word_index.keys())))

dicionarioTeste2 = criar_dicionario(teste_pipeline["data"])
print(dicionarioTeste2)
print(list(dicionarioTeste2.word_index.keys()))
print(len(list(dicionarioTeste2.word_index.keys())))

bagOfWords(dicionarioTeste,bow_dataframe[0])

bagOfWords(dicionarioTeste2,teste_pipeline["data"])

"""## 8.3 Definição e criação do modelo"""

#função de vetorização de Bag of words
def Bow(dados):
  #Recebe os dados a ser realizado o BOW
  dadosTratados = dados
  dadosTratados =  pd.DataFrame({'dadosBow': dadosTratados})
  #geração de dicionario
  dicionario = criar_dicionario(dadosTratados['dadosBow'])
  # chamar o modelo de bag of words
  modelo = (bagOfWords(dicionario,dadosTratados['dadosBow']))
  return modelo,dicionario

#função de vetorização de Bag of words
def Bow_dic(dados, dicionario):
  #Recebe os dados a ser realizado o BOW
  # dadosTratados = dados
  # dadosTratados =  pd.DataFrame({'dadosBow': dadosTratados})
  # chamar o modelo de bag of words
  modelo = (bagOfWords(dicionario,dados[0]))
  return modelo,dicionario

"""## 8.4 Teste do modelo BoW"""

#salvando e exibindo resultados
vetor,dicionario = Bow(teste_pipeline["data"])
dicionario = list(dicionario.word_index.keys())
print(dicionario)
print(vetor)
print(vetor[:, 0])

"""## 8.5 Visualização do modelo"""

#vetor
vetorexportação = vetor[:, 1:]
#para conseguir ver e exportar
dfBOW = pd.DataFrame(vetorexportação, columns=dicionario, index=teste_pipeline)
dfBOW

def contarPalavras(dicionario, matriz):
    dicionario = [''] + dicionario
    contagemPalavras = [0] * len(dicionario)

    for i, palavra in enumerate(dicionario):
        for j in range(len(matriz)):
            contagemPalavras[i] += matriz[j][i]

    contagemDf = pd.DataFrame({'palavra': dicionario, 'repetição': contagemPalavras})

    return contagemDf.iloc[1:]

# Geração da tabela de contagem
contador = contarPalavras(dicionario, vetor)

# Exibição das primeiras 10 palavras mais recorrentes no Bag Of Words
contador.head(10)

"""# 9.0 TF-IDF

Frequência do termo (TF) – é a frequência das palavras em um texto de amostra.

Frequência inversa do documento (IDF) – destaca a frequência das palavras em outros textos de amostra. Os recursos são raros ou comuns nos textos de exemplo é a principal preocupação aqui.

Quando usamos ambos os TF-IDF juntos (TF * IDF), as palavras de alta frequência em um texto de exemplo que tem baixa ocorrência em outros textos de exemplo recebem maior importância.
"""

df_tfidf = pd.DataFrame()

df_tfidf['data'] = df_url['url_noticia'].apply(remove_tags)

df_tfidf['data'] = stopWords(df_tfidf['data'])

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer()

tfidf_matrix = tfidf_vectorizer.fit_transform(df_tfidf['data'])

# Obtém o nome das palavras (termos)
terms = tfidf_vectorizer.get_feature_names_out()

# Calcula a média TF-IDF para cada termo em todas as notícias
tfidf_means = tfidf_matrix.mean(axis=0).tolist()[0]

# Cria um DataFrame com os termos e suas pontuações TF-IDF médias
tfidf_df = pd.DataFrame({'Termo': terms, 'TF-IDF Médio': tfidf_means})

# Classifica o DataFrame por pontuação TF-IDF média em ordem decrescente
tfidf_df = tfidf_df.sort_values(by='TF-IDF Médio', ascending=False)

# Exibe as principais palavras-chave (por exemplo, as 20 principais)
print(tfidf_df.head(20))

"""# 11.0 Word2vec com modelo pré treinado

O Word2Vec é uma técnica de Processamento de Linguagem Natural (PLN) que transforma palavras em vetores numéricos. A principal ideia por trás do Word2Vec é que palavras semelhantes em contexto terão representações vetoriais semelhantes, ou seja, vetores próximos em um espaço multidimensional. Isso permite que as palavras sejam representadas de forma densa e contínua, capturando informações semânticas e relacionamentos entre elas.

## 11.1 CBOW (Continuous Bag of Words)

O W2V é uma técnica que mapeia palavras para vetores densos de números reais. O CBOW é um modelo W2V que prevê a palavra alvo com base em seu contexto, ou seja, palavras próximas a ela em uma frase. Essa técnica captura relações semânticas entre palavras.
"""

teste_pipeline_word2Vec = pipeline_dataframe(df_pipeline, "url_noticia")

teste_pipeline_word2Vec

"""Carregando um modelo pré-treinado de Word2Vec"""

# Word2vec 50 Dimensões
modelo_cbow = KeyedVectors.load_word2vec_format("/content/drive/Shareddrives/CogniVoice/cbow_s50.txt")

"""A função abaico utiliza o modelo pré-treinado Word2Vec para calcular vetores de palavras para cada frase na coluna de entrada, calculando a média dos vetores das palavras em cada frase. Isso pode ser usado para representar as frases em um espaço vetorial contínuo, onde a similaridade entre vetores reflete a semelhança semântica entre as frases."""

def word2Vec(coluna):
  x = []
  for i in range(0,len(coluna)):
    vetor = []
    for h in range(0,len(coluna[i])):
      # Adiciona o vetor da palavra na posição h (índice h) da frase atual no vetor temporário
      vetor.append(modelo_cbow[h])
    # Após percorrer todas as palavras da frase e construir o vetor temporário, este trecho de código calcula a média dos vetores das palavras na frase e adiciona esse vetor médio à lista x.
    x.append(list(map(sum, zip(*vetor))))
    vetor=[]
  print(x)
  return x

testeword2Vec = word2Vec(teste_pipeline_word2Vec['data'])
testeword2Vec[0]
dfTesteword2Vec = pd.DataFrame(testeword2Vec)
dfTesteword2Vec

phrase_input = "Projeto que utiliza inteligência artificial pretende fortalecer línguas indígenas no Brasil envolvendo nossas terras e os europeus"
phrase_input = pipeline_speech_to_text(phrase_input)
phrase_input

testeword2Vecc_ai_input = word2Vec(phrase_input)
dfTesteword2Vecc_ai_input = pd.DataFrame(testeword2Vecc_ai_input)
dfTesteword2Vecc_ai_input

"""## 11.2 Word2Vec com Skipgram

Similar ao CBOW, o modelo de Skipgram é uma abordagem no W2V que faz o oposto. Ele prevê as palavras vizinhas com base em uma palavra de destino. Também é eficaz na captura de relações semânticas entre palavras.
"""

# SkipGram 50 Dimensões
modelo_skipgram = KeyedVectors.load_word2vec_format("/content/drive/Shareddrives/CogniVoice/skip_s50.txt")

def word2Vec_skipgram(coluna):
    # Lista que armazena os vetores resultantes de todas as frases
    x = []
    for i in range(len(coluna)):
        # Lista que armazena os vetores das palavras de cada frase
        vetor = []
        for palavra in coluna[i]:
            if palavra in modelo_skipgram:
                vetor.append(modelo_skipgram[palavra])
        if vetor:
             # Soma dos vetores de todas as palavras
            x.append(list(map(sum, zip(*vetor))))
    return x

testeword2Vec_skipgram = word2Vec_skipgram(teste_pipeline_word2Vec['data'])
testeword2Vec_skipgram[0]
dfTesteword2Vec_skipgram = pd.DataFrame(testeword2Vec_skipgram)
dfTesteword2Vec_skipgram

"""# 12.0 Definição da função de embedding

Uma função de embedding é uma técnica utilizada no campo de Processamento de Linguagem Natural (PLN) para mapear elementos discretos, como palavras ou itens, em vetores de números reais (também conhecidos como embeddings). Esses vetores representam as características e relações semânticas entre os elementos no espaço vetorial.

O objetivo principal de uma função de embedding é criar uma representação densa e contínua para os elementos de um vocabulário, de modo que elementos semanticamente semelhantes sejam mapeados para vetores próximos no espaço vetorial. Isso permite que modelos de aprendizado de máquina entendam e processem esses elementos de forma mais eficaz, capturando informações semânticas e relacionamentos entre eles.

Nesse projeto utilizaremos as seguintes função de ebedding com as seguintes técnicas:

1.   Transformers
2.   BERT
"""

from sentence_transformers import SentenceTransformer
from transformers import BertForSequenceClassification

#função para vetorização
def Vetorizacao(dataframe,modelo):
  # Calcula os embeddings para a coluna de texto
  embeddings_liz = modelo.encode(dataframe.tolist())

  # Cria um novo DataFrame com os embeddings
  embeddings_df = pd.DataFrame(embeddings_liz)

  return embeddings_df

"""# 12.0 Transformers vetorização

Os modelos Transformers, como o GPT (Generative Pre-trained Transformer) e o BERT (Bidirectional Encoder Representations from Transformers), são arquiteturas de aprendizado de máquina que revolucionaram o PLN. Eles são capazes de considerar o contexto bidirecional de palavras em uma sentença, capturando informações contextuais de maneira mais eficaz.
"""

vetorizadorTransformers = SentenceTransformer('distiluse-base-multilingual-cased')

vetoresTransformers = Vetorizacao(teste_pipeline_word2Vec['data'], vetorizadorTransformers)

vetoresTransformers

phrase_input_transformers = "Projeto que utiliza inteligência artificial pretende fortalecer línguas indígenas no Brasil envolvendo nossas terras e os europeus"
phrase_input_transformers = pipeline_speech_to_text(phrase_input_transformers)
phrase_input_transformers

breno = vetorizadorTransformers.encode(phrase_input_transformers)

"""# 13.0 Vetorizadro Bert

O BERT é um modelo de linguagem pré-treinado baseado em Transformers que alcançou resultados de estado-da-arte em uma variedade de tarefas de PLN. Ele é treinado para prever palavras em uma sentença, considerando todas as palavras em ambos os lados do contexto, tornando-o altamente eficaz na compreensão do significado contextual das palavras.
"""

vetorizadorBert = SentenceTransformer('bert-base-multilingual-cased')

vetoresBert = Vetorizacao(teste_pipeline_word2Vec['data'], vetorizadorBert)

vetoresBert

phrase_input_bert = "Projeto que utiliza inteligência artificial pretende fortalecer línguas indígenas no Brasil envolvendo nossas terras e os europeus"
phrase_input_bert = pipeline_speech_to_text(phrase_input_bert)
phrase_input_bert

breno_bert = vetorizadorBert.encode(phrase_input_bert)

"""# 14.0 Similaridade do coseno

A função de similaridade do cosseno é uma métrica usada para medir a semelhança entre dois vetores em um espaço multidimensional. Ela calcula o coseno do ângulo entre esses vetores. Quanto mais próximos os vetores estiverem um do outro, menor será o ângulo e, portanto, maior será o valor do cosseno, indicando maior similaridade. Por outro lado, se os vetores estiverem em direções opostas, o cosseno será -1, indicando dissimilaridade completa. O valor da similaridade do cosseno varia de -1 (completamente dissimilar) a 1 (completamente similar), com 0 indicando independência.

Vamos empregar a função de similaridade do cosseno para comparar o input com todas as notícias armazenadas no banco de dados, que também foram vetorizadas. Com os resultados obtidos através dessa função, identificaremos quais notícias são mais semelhantes ao input, permitindo-nos apresentar as notícias relevantes ao usuário.
"""

def calculate_similiarity(vectors: list, phrase_vector: list) -> float:
    text_similarity_array = []

    if str(type(phrase_vector)) != "<class 'numpy.ndarray'>":
      phrase_vector = phrase_vector.values
    else:
      pass

    for index, row in vectors.iterrows():
        np_vector_row = row.values
        np_vector_text = phrase_vector

        product_sum = np.sum(np_vector_row * np_vector_text)

        squared_frase_um = np.square(np_vector_row)
        squared_frase_dois = np.square(np_vector_text)

        sum_of_squares_array1 = np.sum(squared_frase_um)
        sum_of_squares_array2 = np.sum(squared_frase_dois)

        modulo = math.sqrt(sum_of_squares_array1) * math.sqrt(sum_of_squares_array2)

        text_similarity = product_sum / modulo

        text_similarity_array.append(text_similarity)


    return text_similarity_array

"""#15.0 Casos de testes - Similaridade da noticia com input

Nesta seção, realizamos testes de vetorização e cálculo de similaridade utilizando todos os métodos previamente abordados, com o objetivo de determinar qual modelo pré-treinado melhor se adapta à nossa base de dados. É importante destacar que optamos por não incluir o modelo Bag of Words nestes testes, devido às particularidades da nossa base de dados.

Nossa base de dados consiste em notícias frequentemente extensas, o que resulta em um dimensionamento massivo do Bag of Words, tornando-o computacionalmente ineficiente. Além disso, devido à natureza variável das entradas dos usuários, surgiria a necessidade de criar um novo dicionário a cada novo input do usuário, o que seria impraticável em termos de tempo e recursos computacionais.

Portanto, ao realizar esses testes de vetorização e cálculo de similaridade, estamos buscando determinar qual modelo pré-treinado oferece o melhor desempenho em nossa base de dados, considerando tanto a eficácia quanto a eficiência computacional. Essa análise nos ajudará a escolher a abordagem mais adequada para a nossa aplicação específica.

## Definição da tabela e input
"""

df_test_vectors_copy = pd.read_csv('/content/drive/Shareddrives/CogniVoice/ibm_db.csv')
df_test_vectors = pipeline_dataframe(df_test_vectors_copy, "url_noticia")
df_test_vectors

phrase_title_news = "qual a parceria da ibm com a microsoft"
phrase_title_news = pipeline_speech_to_text(phrase_title_news)
phrase_title_news

"""##Casos de testes 1 - Modelo Word2Vec 50 dimensões"""

vector_full_news_w2v = word2Vec(df_test_vectors['data'])
df_full_news_w2v = pd.DataFrame(vector_full_news_w2v)
df_full_news_w2v

vector_phrase_title_news_w2v = word2Vec(phrase_title_news)
df_phrase_title_news_w2v = pd.DataFrame(vector_phrase_title_news_w2v)
df_phrase_title_news_w2v

calculate_similiarity(df_full_news_w2v,df_phrase_title_news_w2v)

"""##Casos de testes 2 - Modelo SkipGram 50 dimensões"""

vector_full_news_skipgram = word2Vec_skipgram(df_test_vectors['data'])
df_full_news_skipgram = pd.DataFrame(vector_full_news_skipgram)
df_full_news_skipgram

vector_phrase_title_news_skipgram = word2Vec_skipgram(phrase_title_news)
df_phrase_title_news_skipgram = pd.DataFrame(vector_phrase_title_news_skipgram)
df_phrase_title_news_skipgram

calculate_similiarity(df_full_news_skipgram,df_phrase_title_news_skipgram)

"""##Casos de testes 3 - Modelo Transformers"""

df_full_news_transformers = Vetorizacao(df_test_vectors['data'], vetorizadorTransformers)

vector_phrase_title_news_transformers = vetorizadorTransformers.encode(phrase_title_news)

calculate_similiarity(df_full_news_transformers, vector_phrase_title_news_transformers)

"""##Casos de testes 4 - Modelo Bert"""

full_news_bert = Vetorizacao(df_test_vectors['data'], vetorizadorBert)
full_news_bert

vector_phrase_title_news_bert = vetorizadorBert.encode(phrase_title_news)

uau = calculate_similiarity(full_news_bert,vector_phrase_title_news_bert)
uau

"""#16.0 Testes das funções com resultado esperado

Aqui, conduzimos testes abrangentes em todas as funções, garantindo que os resultados estejam em conformidade com as expectativas estabelecidas para cada uma delas. É crucial destacar que esses testes abrangem todas as funcionalidades até a seção de Técnicas de Processamento de Linguagem Natural (PLN). No entanto, vale ressaltar que as funções de vetorização e similaridade do cosseno apresentam uma complexidade que torna difícil prever antecipadamente os resultados esperados. Devido a essa complexidade, nossa abordagem se concentra em realizar testes unitários exclusivamente para as funcionalidades de vetorização e similaridade do cosseno, garantindo assim um rigoroso controle de qualidade em relação a essas partes específicas do sistema.
"""

import unittest

class TestStringManipulationFunctions(unittest.TestCase):
    def test_remocaoAcentos(self):
        result = remocaoAcentos("Café")
        self.assertEqual(result, "Cafe")

    def test_lowercase_text(self):
        result = lowercase_text("Hello World")
        self.assertEqual(result, "hello world")

    def test_clean_title(self):
        result = clean_title("Title - Subtitle")
        self.assertEqual(result, "Title")

    def test_remove_marks(self):
        result = remove_marks("Hello, World! This is a test.")
        self.assertEqual(result, "Hello World This is a test")

    def test_remove_aspas_series(self):
        input_data = pd.Series(['"Text 1"', '"Text 2"'])
        result = remove_aspas(input_data)
        expected_result = pd.Series(['Text 1', 'Text 2'])
        self.assertTrue(result.equals(expected_result))

    def test_remove_aspas_string(self):
        input_string = '"This is a string with quotes"'
        result = remove_aspas(input_string)
        expected_result = 'This is a string with quotes'
        self.assertEqual(result, expected_result)

    def test_remove_aspas_other_types(self):
        result = remove_aspas(12345)
        self.assertEqual(result, 12345)

    def test_remove_duplicate_spaces(self):
        result = remove_duplicate_spaces("   Hello   World   ")
        self.assertEqual(result, "Hello World")

    def test_remove_isolated_numbers(self):
        result = remove_isolated_numbers("Text 123 45 6 with numbers")
        self.assertEqual(result, "Text    with numbers")

    def test_extrair_titulo_com_sucesso(self):
        url = "https://www.google.com/"
        resultado = extrair_titulo(url)
        self.assertEqual(resultado, "Google")

    def test_remove_tags_com_sucesso(self):
        url = "https://www.google.com/"
        resultado = remove_tags(url)
        result_compare = 'google unveils its newest pixel phones advertising business how search works our third decade of climate action join us privacy terms settings search settings advanced search your data in search search history search help send feedback dark theme off google apps'
        self.assertEqual(resultado, result_compare)

    def setUp(self):
        self.sample_data = [
            {
                "tag_removed": "Título da Notícia 12/12/2012",
                "titulo": "Título da Notícia"
            }
        ]

    def test_extract_dates(self):
        for data in self.sample_data:
            result = extract_dates(data)
            self.assertIsNotNone(result, "12/12/2012")

    def test_extrair_dominio(self):
        url = "https://exemplo.com/noticia"
        result = extrair_dominio(url)
        self.assertEqual(result, "exemplo.com")

    def test_named_entities_filtred(self):
        coluna = ["Apple foi fundada em 1976."]
        filtro = "ORG"
        result = named_entities_filtred(coluna, filtro)
        self.assertEqual(result, [{'Apple'}])

    def test_named_entities(self):
        coluna = ["Apple foi fundada em 1976."]
        result = named_entities(coluna)
        self.assertEqual(result, [[('Apple', 'ORG')]])

    def test_stopWords(self):
        dados = ["Este é um exemplo de texto com palavras de parada."]
        result = stopWords(dados)
        self.assertEqual(result, ['texto palavras parada .'])

    def test_lematizar(self):
        dados = ["Eu tenho corrido e eles correm."]
        result = lematizar(dados)
        self.assertEqual(result, ['eu ter corrido e eles correr .'])

    def test_tokenizar_texto(self):
        texto = "Isso é um exemplo."
        result = tokenizar_texto(texto)
        self.assertEqual(result, ["Isso", "é", "um", "exemplo", "."])


test_loader = unittest.TestLoader().loadTestsFromTestCase(TestStringManipulationFunctions)

test_runner = unittest.TextTestRunner(verbosity=2)
test_runner.run(test_loader)

"""#17.0 Funções auxiliares para a API

A fim de garantir que possamos recuperar mais de uma notícia em situações em que a similaridade é extraordinariamente alta, foi necessário implementar duas funções auxiliares em nossa API. Essas funções desempenham papéis essenciais na seleção e apresentação de notícias relevantes para o usuário.

1.   Retorna os index´s das noticias com maiores similaridades identificadas
2.   Retorna o conteúdo das noticias, buscando em nosso dataframe

Essas funções trabalham em conjunto para garantir que o usuário receba uma seleção das notícias mais relevantes e semelhantes ao input, melhorando assim a experiência de busca e recuperação de informações em nosso sistema.

##17.1 Retorna os indexs das noticias com maiores similaridades identificadas

Essa função tem a responsabilidade de identificar as notícias mais semelhantes ao input, limitando a saída a um máximo de três notícias. Uma vez localizadas, ela retorna os índices correspondentes dessas notícias no DataFrame que armazena nosso conjunto de notícias.
"""

def encontrar_maiores(array):
    maiores = []
    for i, numero in enumerate(array):
        if numero > 0.85:
            maiores.append([numero, i])

    maiores.sort(reverse=True)

    if len(maiores) >= 3:
        return maiores[:3]
    elif len(maiores) > 0:
        return maiores
    else:
        return 'Nao ha nenhuma noticia correspondente em nosso banco de dados.'

"""##17.2 Retorna o conteúdo das noticias, buscando em nosso dataframe

Essa função, por sua vez, utiliza os índices previamente obtidos pela função anterior. Seu propósito é recuperar o conteúdo completo dessas notícias e organizá-lo em um formato JSON para apresentação ao usuário.
"""

import json

def encontrar_noticias(indices, dataframe, coluna):
    noticias = []

    if str(type(indices)) == "<class 'str'>":
      return 'Nao ha nenhuma noticia correspondente em nosso banco de dados.'

    for i in indices:
        noticias.append(dataframe[coluna][i[1]])

    noticias_dict = {
        "noticia1": noticias[0] if len(noticias) > 0 else None,
        "noticia2": noticias[1] if len(noticias) > 1 else None,
        "noticia3": noticias[2] if len(noticias) > 2 else None
    }

    json_data = json.dumps(noticias_dict)

    return json_data

"""# 18.0 Processamento do database oficial em nossas pipelines e funções

Esse Dataframe é obtido por meio de uma requisição GET ao nosso back-end, que possui conexão com o banco de dados. Este dataframe será utilizado para recuperar as notícias compatíveis com o usuário em nossa API.
"""

url = 'http://10.254.18.13:3001/v1/url/getall'
response = requests.get(url)

if response.status_code == 200:
    data = response.json()
else:
    print('Erro ao obter os dados da API')

df_news_database = pd.DataFrame(data)

df_news_database

df_news_database_vectors = pipeline_dataframe(df_news_database, "kkkk")
df_news_database_vectors

df_full_news_transformers = Vetorizacao(df_news_database_vectors['kkkk'], vetorizadorTransformers)

"""# 19.0 Flask API"""

from flask import Flask, request, jsonify
import requests
import pandas as pd

app = Flask(__name__)

# Rota de saúde
@app.route("/health", methods=['GET'])
def health_check():
    return "API está rodando sem erros", 200

# Rota para processar a entrada de voz
@app.route("/process_audio", methods=['POST'])
def process_audio():
    try:
        data = request.get_json()
        if data is not None and 'text' in data:
            text_value = data['text']

            # Coloque aqui as chamadas das funções relacionadas ao processamento de áudio

            return "Sucesso", 200
        else:
            return jsonify({"erro": "Dados JSON inválidos"}), 400
    except Exception as e:
        return jsonify({"erro": str(e)}), 500

@app.route("/call_external_api", methods=['GET'])
def update_news():
    url = 'http://10.254.18.13:3001/v1/url/getall'
    response = requests.get(url)

    if response.status_code == 200:
        data = response.json()
    else:
        print('Erro ao obter os dados da API')

    df_news_database = pd.DataFrame(data)

    return "acionada em", 200

if __name__ == '__main__':
    app.run(host='44.214.133.236', port=3000)
